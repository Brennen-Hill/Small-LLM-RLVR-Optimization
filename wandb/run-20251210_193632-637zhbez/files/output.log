[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
INFO:__main__:Loading and processing datasets...
INFO:data_processing:Loading MBPP dataset...
INFO:data_processing:Original MBPP columns: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list']
INFO:data_processing:Original MBPP size: 974
INFO:data_processing:Processed MBPP columns: ['prompt', 'completion']
INFO:data_processing:Processed MBPP size: 9
INFO:data_processing:Loading APPS dataset...
INFO:data_processing:Original APPS columns: ['problem_id', 'question', 'solutions', 'input_output', 'difficulty', 'url', 'starter_code']
INFO:data_processing:Original APPS size: 5000
INFO:data_processing:Processed APPS columns: ['prompt', 'completion']
INFO:data_processing:Processed APPS size: 50
INFO:__main__:MBPP dataset size: 9
INFO:__main__:APPS dataset size: 50
INFO:__main__:Skipping SFT warmup, using base model for GRPO
INFO:__main__:==================================================
INFO:__main__:Starting GRPO Training
INFO:__main__:==================================================
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
  0%|                                                                                                         | 0/150 [00:00<?, ?it/s]Traceback (most recent call last):
{'completion_ids': [[576, 4396, 6291, 686, 387, 2686, 1091, 220, 16, 15, 82, 624, 334, 97611, 106185, 1019, 13874, 3989, 947, 38636, 14778, 284, 729, 320, 1118, 317, 262, 1077, 2790, 284, 220, 15, 280, 262, 421, 320, 1118, 621, 845, 340, 262, 341, 286, 470, 220, 15, 280, 262, 456, 262, 369, 320, 1149, 600, 284, 220, 15, 26, 600, 27, 2890, 13, 3084, 26, 600, 3485, 262, 341, 286, 2790, 1421, 4149, 14572, 10939, 989, 60, 481, 2890, 989, 488, 16, 2558, 262, 456, 73594, 256, 220, 104465, 4149, 14572, 2075, 69515, 80158, 100710, 118274, 101149, 3837, 104689, 102173, 104552, 118274, 3837, 107525, 99796, 3407, 334, 100005, 46100, 1019, 13874, 3989, 947, 38636, 14778, 284, 729, 320, 1653, 8, 341, 262, 1077, 2790, 284, 220, 15, 280, 262, 369, 320, 1149, 600, 284, 220, 15, 26, 600, 27, 1334, 1954, 26, 600, 3485, 262, 341, 286, 2790, 488, 1334, 989, 60, 481, 1334, 989, 488, 16, 935, 262, 456, 262, 470, 2790, 280, 2440, 73594, 151643], [4354, 421, 697, 4688, 5221, 432, 1290, 369, 678, 315, 1493, 498, 686, 1083, 5258, 8311, 6668, 369, 3281, 12037, 7032, 382, 39, 3221, 25, 61830, 7132, 504, 6422, 311, 6422, 53724, 498, 279, 1852, 2783, 438, 43084, 11450, 25165, 39296, 382, 565, 12478, 220, 16, 320, 18, 14, 18, 15, 7731, 262, 762, 38636, 14778, 284, 729, 38502, 1072, 8, 314, 198, 262, 762, 6010, 284, 220, 15, 26, 198, 262, 369, 320, 762, 600, 284, 220, 15, 26, 600, 366, 458, 1072, 1954, 481, 220, 16, 26, 600, 2457, 314, 5872, 286, 6010, 1421, 4149, 14572, 38502, 1072, 989, 60, 481, 458, 1072, 989, 488, 220, 16, 2546, 262, 456, 262, 470, 6010, 198, 262, 555, 565, 12478, 220, 17, 320, 16, 14, 18, 15, 7731, 262, 729, 38636, 14778, 6110, 8, 341, 2405, 6010, 284, 220, 15, 280, 1572, 2023, 7537, 600, 284, 220, 15, 26, 600, 366, 1334, 1954, 26, 600, 6796, 197, 98052, 1421, 4149, 14572, 1956, 481, 320, 1653, 1954, 481, 220, 16, 1106, 197, 532, 853, 6010, 26, 715, 262, 335, 151643]], 'trainer_state': TrainerState(epoch=0, global_step=0, max_steps=150, logging_steps=10, eval_steps=500, save_steps=500, train_batch_size=2, num_train_epochs=3, num_input_tokens_seen=747, total_flos=0, log_history=[], best_metric=None, best_global_step=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})}
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 371, in <module>
    main()
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 351, in main
    run_grpo_training(
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 236, in run_grpo_training
    trainer.train()
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2316, in train
    return inner_training_loop(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 4014, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1067, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1594, in _generate_and_score_completions
    rewards_per_func = self._calculate_rewards(inputs, prompts, completions, completion_ids_list)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1112, in _calculate_rewards
    output_reward_func = reward_func(
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 95, in bleu_reward_function
    references_raw = kwargs["completion"]
KeyError: 'completion'
Traceback (most recent call last):
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 371, in <module>
    main()
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 351, in main
    run_grpo_training(
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 236, in run_grpo_training
    trainer.train()
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2316, in train
    return inner_training_loop(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 4014, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1067, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1594, in _generate_and_score_completions
    rewards_per_func = self._calculate_rewards(inputs, prompts, completions, completion_ids_list)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1112, in _calculate_rewards
    output_reward_func = reward_func(
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 95, in bleu_reward_function
    references_raw = kwargs["completion"]
KeyError: 'completion'
