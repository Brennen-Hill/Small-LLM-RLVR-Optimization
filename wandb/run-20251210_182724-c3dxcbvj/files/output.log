[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
INFO:__main__:Loading and processing datasets...
INFO:data_processing:Loading MBPP dataset...
INFO:data_processing:Original MBPP columns: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list']
INFO:data_processing:Original MBPP size: 974
INFO:data_processing:Processed MBPP columns: ['prompt', 'completion']
INFO:data_processing:Processed MBPP size: 9
INFO:data_processing:Loading APPS dataset...
INFO:data_processing:Original APPS columns: ['problem_id', 'question', 'solutions', 'input_output', 'difficulty', 'url', 'starter_code']
INFO:data_processing:Original APPS size: 5000
INFO:data_processing:Processed APPS columns: ['prompt', 'completion']
INFO:data_processing:Processed APPS size: 50
INFO:__main__:MBPP dataset size: 9
INFO:__main__:APPS dataset size: 50
INFO:__main__:==================================================
INFO:__main__:Starting SFT Warmup Training
INFO:__main__:==================================================
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:09<00:00, 13.90s/it]
{'train_runtime': 69.5228, 'train_samples_per_second': 0.129, 'train_steps_per_second': 0.072, 'train_loss': 0.7285871505737305, 'entropy': 1.0010419368743897, 'num_tokens': 693.0, 'mean_token_accuracy': 0.7839648485183716, 'epoch': 1.0}
INFO:__main__:SFT model saved to ./outputs/sft_checkpoint
INFO:__main__:==================================================
INFO:__main__:Starting GRPO Training
INFO:__main__:==================================================
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 78.08it/s]
The tokenizer you are loading from './outputs/sft_checkpoint' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 65/150 [08:13<10:52,  7.67s/it]Traceback (most recent call last):
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.399999999999999e-07, 'num_tokens': 10313.0, 'completions/mean_length': 190.55, 'completions/min_length': 135.1, 'completions/max_length': 246.0, 'completions/clipped_ratio': 0.55, 'completions/mean_terminated_length': 91.65, 'completions/min_terminated_length': 83.9, 'completions/max_terminated_length': 99.4, 'rewards/bleu_reward_function/mean': 0.0, 'rewards/bleu_reward_function/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.21040478348732, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.2}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.733333333333333e-07, 'num_tokens': 21141.0, 'completions/mean_length': 202.9, 'completions/min_length': 149.8, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.75, 'completions/mean_terminated_length': 21.8, 'completions/min_terminated_length': 21.8, 'completions/max_terminated_length': 21.8, 'rewards/bleu_reward_function/mean': 0.0, 'rewards/bleu_reward_function/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.3121758908033372, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.4}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.066666666666666e-07, 'num_tokens': 31096.0, 'completions/mean_length': 151.75, 'completions/min_length': 98.0, 'completions/max_length': 205.5, 'completions/clipped_ratio': 0.45, 'completions/mean_terminated_length': 36.7, 'completions/min_terminated_length': 21.2, 'completions/max_terminated_length': 52.2, 'rewards/bleu_reward_function/mean': 0.0, 'rewards/bleu_reward_function/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.3769485712051392, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.6}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.4e-07, 'num_tokens': 39875.0, 'completions/mean_length': 162.35, 'completions/min_length': 132.5, 'completions/max_length': 192.2, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 61.5, 'completions/min_terminated_length': 55.7, 'completions/max_terminated_length': 67.3, 'rewards/bleu_reward_function/mean': 0.0, 'rewards/bleu_reward_function/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.1904352486133576, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.8}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.733333333333333e-07, 'num_tokens': 49230.0, 'completions/mean_length': 204.75, 'completions/min_length': 177.1, 'completions/max_length': 232.4, 'completions/clipped_ratio': 0.55, 'completions/mean_terminated_length': 81.2, 'completions/min_terminated_length': 74.7, 'completions/max_terminated_length': 87.7, 'rewards/bleu_reward_function/mean': 0.0, 'rewards/bleu_reward_function/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.5067261099815368, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.0}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.066666666666666e-07, 'num_tokens': 59306.0, 'completions/mean_length': 176.2, 'completions/min_length': 132.3, 'completions/max_length': 220.1, 'completions/clipped_ratio': 0.6, 'completions/mean_terminated_length': 32.4, 'completions/min_terminated_length': 29.9, 'completions/max_terminated_length': 34.9, 'rewards/bleu_reward_function/mean': 0.0, 'rewards/bleu_reward_function/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.54668185710907, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.2}
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 352, in <module>
    main()
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 332, in main
    run_grpo_training(
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 218, in run_grpo_training
    trainer.train()
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2316, in train
    return inner_training_loop(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 4014, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1067, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1462, in _generate_and_score_completions
    self._generate(prompts)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1400, in _generate
    prompt_ids, completion_ids, logprobs, extra_fields = self._generate_single_turn(prompts)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1375, in _generate_single_turn
    prompt_completion_ids = unwrapped_model.generate(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 480, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 410, in forward
    hidden_states = decoder_layer(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 260, in forward
    hidden_states, _ = self.self_attn(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1956, in __getattr__
    if name in _buffers:
KeyboardInterrupt
Traceback (most recent call last):
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 352, in <module>
    main()
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 332, in main
    run_grpo_training(
  File "/hdd3/albertwu/mini_rlvr/train_grpo_single.py", line 218, in run_grpo_training
    trainer.train()
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2316, in train
    return inner_training_loop(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/trainer.py", line 4014, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1067, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1462, in _generate_and_score_completions
    self._generate(prompts)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1400, in _generate
    prompt_ids, completion_ids, logprobs, extra_fields = self._generate_single_turn(prompts)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1375, in _generate_single_turn
    prompt_completion_ids = unwrapped_model.generate(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 480, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 410, in forward
    hidden_states = decoder_layer(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py", line 260, in forward
    hidden_states, _ = self.self_attn(
  File "/hdd3/albertwu/miniconda3/envs/rlvr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1956, in __getattr__
    if name in _buffers:
KeyboardInterrupt
